| Name                   | Description                                                                                                                                                           | Link(s)                                                                                                  | Open Source | Key Agentic Aspects Tested                                                                                                |
| :--------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------- | :---------- | :------------------------------------------------------------------------------------------------------------------------ |
| **AgentBench** | A comprehensive benchmark from THUDM evaluating LLMs as agents across 8 diverse environments (OS, DBs, KG, games, puzzles, household tasks).                            | [GitHub](https://github.com/THUDM/AgentBench)                                                            | Yes         | General Agent Capabilities, Planning, Reasoning, Task Completion in Diverse Environments                                  |
| **REALM-Bench** | (Realistic Embodied ALM Benchmark) Evaluates embodied AI agents in realistic, interactive 3D environments, focusing on perception, planning, and navigation tasks.        | [GitHub](https://github.com/genglongling/REALM-Bench)                                                    | Yes         | Embodied Interaction, Perception, Planning, Navigation in 3D Environments                                                 |
| **BrowserGym** | A framework from ServiceNow for developing and evaluating web-based AI agents, enabling interaction with live or simulated websites for task completion.                  | [GitHub](https://github.com/ServiceNow/BrowserGym)                                                       | Yes         | Web Navigation, Web Task Completion, Human-Computer Interaction Simulation                                              |
| **Ï„-Bench (Tau Bench)**| From Sierra Research, evaluates language model-based agents on complex, multi-turn tasks requiring consistent interaction, planning, and tool use (e.g., customer service).   | [GitHub](https://github.com/sierra-research/tau-bench), [Paper](https://huggingface.co/papers/2406.12045) | Yes         | Multi-turn Interaction, Consistency, Planning, Tool Use (in conversational/service contexts)                             |
| **WebArena** | A realistic and reproducible web environment benchmark for testing autonomous agents on complex web tasks, focusing on instruction following and goal achievement.        | [GitHub](https://github.com/web-arena-x/webarena)                                                        | Yes         | Web Navigation, Instruction Following, Autonomous Task Completion on Web                                                |
| **ToolBench** | From OpenBMB, evaluates LLMs' ability to use a diverse range of tools (APIs) with a large dataset and evaluation framework.                                            | [GitHub](https://github.com/OpenBMB/ToolBench)                                                           | Yes         | Tool Use Proficiency, API Interaction, Planning with Tools                                                              |
| **AgentBoard** | An evaluation board/framework for comprehensive and systematic assessment of LLM-based agents, focusing on diverse capabilities and complex tasks.                        | [GitHub](https://github.com/agentboard/agentboard)                                                       | Yes         | General Agent Capabilities, Complex Task Solving, Multi-faceted Agent Performance                                         |
| **ALFWorld** | An instruction-following benchmark where agents complete complex, multi-step household tasks in a simulated text-based environment by mapping instructions to actions.   | [GitHub](https://github.com/alfworld/alfworld)                                                           | Yes         | Instruction Following, Multi-step Task Completion, Planning (in text-based embodied environments)                        |
| **AutoGPT** | An experimental open-source application showcasing GPT-4's capabilities to act autonomously. It's a platform for autonomous agents that is often a *target* for benchmarking efforts. | [GitHub (Significant Gravitas)](https://github.com/Significant-Gravitas/AutoGPT)                         | Yes         | (Platform for) Autonomous Operation, Planning, Tool Use, Complex Task Decomposition (when benchmarked by other frameworks) |
| **WebShop** | An interactive e-commerce website environment designed to *enable the benchmarking* of AI agents on complex, realistic web-based tasks like shopping.                 | [GitHub (princeton-nlp)](https://github.com/princeton-nlp/webshop)                                       | Yes         | (Environment for) Web Navigation, Decision Making, Instruction Following in E-commerce                                  |
| **Big-Bench (BB)** | A large, collaborative Google benchmark with diverse tasks designed to probe and evaluate the capabilities and limitations of language models, many requiring complex reasoning.   | [GitHub](https://github.com/google/BIG-bench)                                                            | Yes         | Complex Reasoning, Diverse Task Capabilities (Foundational LLM skills for agents)                                       |
| **BetterBench** | From Stanford, aims to create more robust and realistic benchmarks for evaluating general-purpose AI systems, including aspects of agentic behavior.                    | [Website](https://betterbench.stanford.edu/)                                                             | Yes         | General-purpose AI Evaluation, Robustness, Agentic Behavior Aspects                                                       |
| **DoomArena** | A modular, configurable, plug-in framework by ServiceNow for security testing of AI agents against evolving threats using an "Attack Gateway" concept.                | [GitHub](https://github.com/ServiceNow/DoomArena)                                                        | Yes         | Agent Security, Robustness to Attacks, Threat Simulation, Safety                                                        |
| **CLEVA** | (Continual Learning EValuation for Agents) A benchmark for evaluating the continual learning capabilities of autonomous agents in 3D environments.                       | [GitHub (clvrai)](https://github.com/clvrai/cleva)                                                       | Yes         | Continual Learning for Autonomous Agents, Adaptability in 3D Environments                                               |
| **GAIA** | A benchmark for General AI Assistants requiring complex reasoning, multi-hop lookups, and tool use, designed to be challenging for current LLMs.                        | [Paper (on Hugging Face)](https://huggingface.co/papers/2311.12983)                                       | Yes (Data)  | Complex Reasoning, Multi-hop Information Retrieval, Tool Use (for General AI Assistants)                                |
| **OpenAI Evals** | A framework for creating/running evaluations on AI models and a registry for open-source evals. Many test reasoning & instruction following crucial for agents.           | [GitHub](https://github.com/openai/evals)                                                              | Yes         | Reasoning, Instruction Following, Safety (Foundational LLM skills for agents)                                           |
| **HELM (Holistic Eval)**| From Stanford CRFM, a comprehensive benchmark for LLMs. Evaluates foundational capabilities (reasoning, instruction following, safety) crucial for effective and safe AI agents. | [Website](https://crfm.stanford.edu/helm/latest/)                                                        | Yes         | Foundational LLM Capabilities for Agents (reasoning, instruction following, safety, robustness)                         |
| **Mind2Web** | A benchmark for evaluating generalist agents on realistic web tasks, involving following natural language instructions on diverse websites.                            | [GitHub (OSU-NLP-Group)](https://github.com/OSU-NLP-Group/Mind2Web)                                      | Yes         | Web Navigation, Generalist Agent Performance, Instruction Following on Web                                              |
| **InterCode** | An interactive code generation benchmark where LLM agents interact with code environments (Bash, Python, SQL) to solve complex, context-dependent tasks.             | [GitHub (princeton-nlp)](https://github.com/princeton-nlp/intercode)                                     | Yes         | Code Generation as Tool Use, Interaction with Code Environments, Problem Solving                                        |
| **Mobile-Env** | A platform for evaluating embodied AI agents that interact with mobile device UIs by tapping/swiping on realistic mobile app interfaces.                            | [GitHub (google-research)](https://github.com/google-research/mobile-env)                                | Yes         | Mobile UI Interaction, Task Completion on Mobile Devices, Embodied Interaction (Digital)                                |
| **OpenAgents** | A platform for building and evaluating LLM-based agents, focusing on replicating real-world applications and user interactions, supporting tool use and web Browse. | [GitHub (xlang-ai)](https://github.com/xlang-ai/OpenAgents)                                              | Yes         | Real-world Application Replication, Tool Use, Web Browse, User Interaction Simulation                                 |

*(Note: The "Open Source" status is based on the primary repositories. Licensing details should always be checked directly from the source.)*
